% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions 
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2".
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

\usepackage{color,soul}
\usepackage[normalem]{ulem}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{commath}

\usepackage{mhchem}
\usepackage{tikz}
\usepackage{mathtools}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}


% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

\usepackage{makecell}
\usepackage{gensymb}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{multirow}


%% Include all macros below
\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}
\newcommand{\angstrom}{\mbox{\normalfont\AA}}

\newenvironment{changemargin}[2]{%
\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{#1}%
\setlength{\rightmargin}{#2}%
\setlength{\listparindent}{\parindent}%
\setlength\textit{emindent}{\parindent}%
\setlength{\parsep}{\parskip}%
}%
\item[]}{\end{list}}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 


\renewcommand{\arraystretch}{1.3}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Adaptive dating and fast proposals: revisiting the phylogenetic relaxed clock model} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Jordan Douglas\textsuperscript{1,2*},
Rong Zhang\textsuperscript{1,2},
Remco Bouckaert\textsuperscript{1,2,3}
\\
\bigskip
\textbf{1} Centre for Computational Evolution,  University of Auckland, Auckland, New Zealand\\
\textbf{2} School of Computer Science, University of Auckland, Auckland, New Zealand\\
\textbf{3} Max Planck Institute for the Science of Human History, Jena, Germany\\
\bigskip


% Use the asterisk to denote corresponding authorship and provide email address in note below.
* jordan.douglas@auckland.ac.nz


\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}


% What is the relaxed clock
% It is widely used
% We delve deep into its complexities to develop efficient operators
% Weight learning adaptive operator
% Rate parameterisations
% Other modern methods Bactrian and AVMVN kernels applied to the relaxed clock
% Overall, up to 2 orders of magnitude faster at exploring continuous relaxed clock parameters 
% Topological operators for the relaxed clock -> up to 40%
% Methods are best for long alignments

%Molecular clock models are used for dating evolutionary divergence times. 
Uncorrelated relaxed clock models enable estimation of molecular substitution rates across lineages and are widely used in phylogenetics for dating evolutionary divergence times.
In this article we delved into the internal complexities of the relaxed clock model in order to develop efficient MCMC operators for Bayesian phylogenetic inference.
We compared three substitution rate parameterisations, introduced an adaptive operator which learns the weights of other operators during MCMC, and we explored how relaxed clock model estimation can benefit from two cutting-edge proposal kernels: the AVMVN and Bactrian kernels.
This work has produced an operator scheme that is up to 65 times more efficient at exploring continuous relaxed clock parameters compared with previous setups, depending on the dataset. 
Finally, we explored variants of the standard narrow exchange operator which are specifically designed for the relaxed clock model.
In the most extreme case, this new operator traversed tree space 40\% more efficiently than narrow exchange.
The methodologies introduced are adaptive and highly effective on short as well as long alignments.
The results are available via the open source optimised relaxed clock (ORC) package for BEAST 2 under a GNU licence (\url{https://github.com/jordandouglas/ORC}).

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.   
\section*{Author summary}

Biological sequences, such as DNA, accumulate mutations over generations. 
By comparing such sequences in a phylogenetic framework, the evolutionary tree of lifeforms can be inferred.
With the overwhelming availability of biological sequence data, and the increasing affordability of collecting new data, the development of fast and efficient phylogenetic algorithms is more important than ever.
In this article we focus on the relaxed clock model, which is very popular in phylogenetics.
We explored how a range of optimisations can improve the statistical inference of the relaxed clock.
This work has produced a phylogenetic setup which can infer parameters related to the relaxed clock up to 65 times faster than previous setups, depending on the dataset. 
The methods introduced adapt to the dataset during computation and are highly efficient when processing long biological sequences.    


\linenumbers

\clearpage
% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}


The molecular clock hypothesis states that the evolutionary rates of biological sequences are approximately constant through time \cite{zuckerkandl1962molecular}.
This assumption forms the basis of phylogenetics, under which the evolutionary trees and divergence dates of life forms are inferred from biological sequences, such as nucleic and amino acids \cite{douzery2003local, drummond2006relaxed}.
In Bayesian phylogenetics, these trees and their associated parameters are estimated as probability distributions \cite{kuhner1995estimating, larget1999markov, mau1999bayesian}. 
Statistical inference can be performed by the Markov chain Monte Carlo (MCMC) algorithm \cite{metropolis53, hastings70} using platforms such as BEAST \cite{drummond2012bayesian}, BEAST 2 \cite{bouckaert2019beast}, MrBayes \cite{ronquist2012mrbayes}, and RevBayes \cite{hohna2016revbayes}.



The simplest phylogenetic clock model -- the strict clock -- makes the mathematically convenient assumption that the evolutionary rate is constant across all lineages \cite{zuckerkandl1965evolutionary, kuhner1995estimating, larget1999markov}.  % Early examples? 
However, molecular substitution rates are known to vary over time, over population sizes, over evolutionary pressures, and over nucleic acid replicative machineries \cite{gillespie1994causes, woolfit2009effective, loh2010optimization}.
Moreover, any given dataset could be clock-like (where substitution rates have a small variance across lineages) or non clock-like (a large variance). 
In the latter case, a strict clock is probably not suitable. 



This led to the development of relaxed (uncorrelated) clock models, under which each branch in the phylogenetic tree has its own molecular substitution rate  \cite{drummond2006relaxed}.
Branch rates can be drawn from a range of probability distributions including log-normal, exponential, gamma, and inverse-gamma distributions \cite{drummond2006relaxed, lepage2007general, li2012model}.
This class of models is widely used, and has aided insight into many recent biological problems, including the 2016 Zika virus outbreak \cite{faria2017establishment} and the COVID-19 pandemic \cite{giovanetti2020first}.
In the remainder of this paper we will only consider uncorrelated relaxed clock models.
% is compatible with increasingly prominent multispecies coalescent models \cite{ogilvie2017starBEAST 2}, 

Finally, although not the focus of this article, the class of correlated clock models assumes some form of auto-correlation between rates over time. 
The correlation itself can invoke a range of stochastic models, including compound Poisson \cite{huelsenbeck2000compound} and CIR processes \cite{lepage2007general}, or it can exist as a series of local clocks \cite{drummond2010bayesian}. 
However, due to the correlated and discrete nature of such models, the time required for MCMC to achieve convergence can be cumbersome, particularly for larger datasets \cite{drummond2010bayesian}.  



With the overwhelming availability of biological sequence data, the development of efficient Bayesian phylogenetic methods is more important than ever. 
The performance of MCMC is dependent not only on computational performance but also the efficacy of an
MCMC setup to achieve convergence.
A critical task therein lies the further advancement of MCMC operators.
Recent developments in this area include the advancement of guided tree proposals \cite{zhang2020using, meyer2019adaptive, hohna2012guided}, coupled MCMC \cite{altekar2004parallel, muller2019coupled},
adaptive multivariate transition kernels \cite{baele2017adaptive},
and other explorative proposal kernels such as the Bactrian and mirror kernels \cite{yang2013searching, thawornwattana2018designing}.
In the case of relaxed clocks, informed tree proposals can account for correlations between substitution rates and divergence times \cite{zhang2020improving}. 
The rate parameterisation itself can also affect the ability to ``mix'' during MCMC \cite{drummond2006relaxed, li2012model, zhang2020improving}. 




While a range of advanced operators and other MCMC optimisation methods have arisen over the years, there has yet to be a large scale performance benchmarking of such methods as applied to the relaxed clock model. 
In this article, we systematically evaluate how the relaxed clock model can benefit from i) adaptive operator weighting, ii) substitution rate parameterisation,  iii) Bactrian proposal kernels \cite{yang2013searching}, iv) tree operators which account for correlations between substitution rates and times, and v) adaptive multivariate operators \cite{baele2017adaptive}.
The discussed methods are implemented in the ORC package and compared using BEAST 2 \cite{bouckaert2019beast}.

% phylogenetic model -- and the operators used during MCMC -- to achieve convergence. 
%A great body of research of MCMC performance



% With increasing sequence availability, it is desirable to have faster MCMC
% This can be achieved through faster mixing and better performance (beagle)
% Address modern optimisation speedups - bactrian, AVMVN, + 2 others not covered in this paper including GUIDED tree proposals
% Address various attempts made to optimise clock models (Rong's paper, different parameterisations + 1 other not covered)
% While many ideas have come up over the years, a large scale systematic comparison of which methods yield faster mixing (if there is one) has not been applied


% In this article, benchmark a range of approaches including...





% Results and Discussion can be combined.
\section*{Models and Methods} \label{sect:models}



\subsection*{Preliminaries}



Let $\mathcal{T}$ be a binary rooted time tree with $N$ taxa, and %. Let $L$ be the number of sites within the multiple sequence alignment $D$. 
data $D$ associated with the tips, such as a multiple sequence alignment with $L$ sites, morphological data or geographic locations. 
The posterior density of a phylogenetic model is described by

\begin{eqnarray}
\label{eq:bayesian}
p(\mathcal{T}, \vec{\mathcal{R}}^{\,}, \sigma, \mu_C, \theta|D) \propto  p(D|\mathcal{T}, r(\vec{\mathcal{R}}^{\,}), \mu_C, \theta) \; p(\mathcal{T}|\theta) \;  p(\vec{\mathcal{R}}^{\,} | \sigma) \; p(\sigma) \; p(\mu_C) \; p(\theta)
\end{eqnarray}

\noindent
where $\sigma$ and $\mu_C$ represent clock model related parameters, and $p(\mathcal{T}|\theta)$ is the tree prior where $\theta$ describes further unspecified parameters. 
The tree likelihood $p(D|\mathcal{T}, r(\vec{\mathcal{R}}^{\,}), \mu_C,\theta)$ %is calculated by the tree-pruning algorithm \cite{felsenstein1981evolutionary}, where
has $\mu_C$ as the overall clock rate and
$\vec{\mathcal{R}}^{\,}$ is an abstracted vector of branch rates which is transformed into real rates by function $r(\vec{\mathcal{R}}^{\,})$. 
Branch rates have a mean of 1 under the prior to avoid non-identifiability with the clock rate $\mu_C$.
Three methods of representing rates as $\vec{\mathcal{R}}^{\,}$ are presented in \textbf{\nameref{sect:rateparams}}.  




Let $t_i$ be the height (time) of node $i$.
Each node $i$ in $\mathcal{T}$, except for the root, is associated with a parental branch length $\tau_i$ (the height difference between $i$ and its parent)  and a parental branch substitution rate $r_i = r(\mathcal{R}_i)$. 
In a relaxed clock model, each of the $2N-2$ elements in $\vec{\mathcal{R}}^{\,}$ are independently distributed under the prior $p(\vec{\mathcal{R}}^{\,} | \sigma)$.



The posterior distribution is sampled by the Metropolis-Hastings-Green MCMC algorithm \cite{metropolis53, hastings70, green1995reversible},
under which the probability of accepting proposed state $x^\prime$ from state $x$ is equal to:

\begin{eqnarray}
\label{eq:MCMC}
\alpha(x^\prime|x) =  \min\left( 1, \frac{p(x^\prime|D)}{p(x|D)} \; \frac{q(x|x^\prime)}{q(x^\prime|x)} \; |J| \right)
\end{eqnarray}

\noindent
where $q(a|b)$ is the transition kernel: the probability of proposing state $b$ from state $a$.
The ratio between the two $\frac{q(x|x^\prime)}{q(x^\prime|x)}$ is known as the Hastings ratio \cite{hastings70}.
The determinant of the Jacobian matrix $|J|$, known as the Green ratio, solves the dimension-matching problem for proposals which operate on multiple terms across one or more spaces \cite{green1995reversible, geyer2003metropolis}. 
%This term is known as the Green ratio. 




\clearpage
\subsection*{Branch rate parameterisations}
\label{sect:rateparams}

In Bayesian inference, the way parameters are represented in the model can affect the mixing ability of the model and the meaning of the model itself \cite{gelman2004parameterization}. Three methods for parameterising substitution rates are described below.
 Each parameterisation is associated with i) an abstraction of the branch rate vector $\vec{\mathcal{R}}^{\,}$, ii) some function for transforming this parameter into unabstracted branch rates $r(\vec{\mathcal{R}}^{\,})$, and iii) a prior density function of the abstraction $p(\vec{\mathcal{R}}^{\,} | \sigma) $. 
 The three methods are summarised in \textbf{Fig \ref{fig:rateparams}}.


\begin{figure}[!h]
\includegraphics[width=\textwidth]{Figures/rateparameterisation.pdf}
\caption{\textbf{Branch rate parameterisations.} Top left: the prior density of a branch rate $r$ under a Log-normal($-0.5\sigma^2, \sigma$) distribution (with its mean fixed at 1). 
The function for transforming $\mathcal{R}$ into branch rates $r(\mathcal{R})$ is depicted for \textit{real} (top right), \textit{cat} (bottom left), and \textit{quant} (bottom right).
For visualisation purposes, there are only 10 bins/pieces displayed, however in practice we use $2N-2$ bins for \textit{cat} and 100 pieces for \textit{quant}.
The first and final \textit{quant} pieces are equal to the underlying function (solid lines) however the pieces in between use linear approximations of this function (dashed lines).}
\label{fig:rateparams}
\end{figure}



\clearpage
\subsubsection*{1. Real rates}
The natural (and unabstracted) parameterisation of a substitution rate is a real number $\mathcal{R}_i \in \mathbb{R}, \mathcal{R}_i > 0$ which is equal to the rate itself. 
Under the \textit{real} parameterisation:

% \vec{\mathcal{R}}^{\,} =& \; \vec{r}^{\,} \\
\begin{align}
r(\vec{\mathcal{R}}^{\,}) =& \; \vec{\mathcal{R}}^{\,}.
\end{align}


Under a log-normal clock prior $p(\vec{\mathcal{R}}^{\,} | \sigma)$, rates are distributed with a mean of 1:

\begin{align}
p(\mathcal{R}_i | \sigma) = \frac{1}{\mathcal{R}_i \sigma \sqrt{2\pi}} \exp \big( -\frac{(\ln \mathcal{R}_i - \mu)^2}{2\sigma^2} \big) 
\end{align}


\noindent
where $\mu = -0.5\sigma^2$ is set such that the expected value is 1.
In this article we only consider log-normal clock priors, however the methods discussed are general.


Zhang and Drummond 2020 introduced a series of tree operators which propose node heights and branch rates, such that the resulting genetic distances ($r_i \times \tau_i$) remain constant \cite{zhang2020improving}. 
These operators account for correlations between branch rates and branch times.
By keeping the genetic distance of each branch constant, the likelihood is unaltered by the proposal. 


%the correlation between rates and times, as imposed by likelihood function, is r after the proposal , and thus lowers the risk of ``falling o

% Such operators are incompatible with the \textit{cat} parameterisation.  The \textit{real} parameterisation and its associated operators yield a mixing rate up to an order of magnitude faster than that of \textit{cat}. \\




\subsubsection*{2. Categories}
The category parameterisation \textit{cat} is an abstraction of the \textit{real} parameterisation. 
Each of the $2N-2$ branches are assigned an integer from $0$ to $n-1$:

\begin{align}
\vec{\mathcal{R}}^{\,} \in& \{ 0, 1, \dotso, n-1 \}^{2N-2}.
\end{align}


These integers correspond to $n$ rate categories (\textbf{Fig. \ref{fig:rateparams}}).
Let $f(x|\sigma)$ be the probability density function (PDF) and let $F(x|\sigma) = \int\limits_{0}^{x} f(t|\sigma) \; dt$ be the cumulative distribution function (CDF) of the prior distribution used by the underlying \textit{real} clock model (a log-normal distribution for the purposes of this article). 
In the \textit{cat} parameterisation, $f(x|\sigma)$ is discretised into $n$ bins and each element within $\vec{\mathcal{R}}^{\,}$ points to one such bin.
The rate of each bin is equal to its median value:


\begin{align}
r(\mathcal{R}_i) =& \; F^{-1}\big(\frac{\mathcal{R}_i + 0.5}{n}\big),
\end{align}

\noindent
where $F^{-1}$ is the inverse cumulative distribution function (i-CDF).
The domain of $\vec{\mathcal{R}}^{\,}$ is uniformly distributed under the prior:


\begin{align}
p(\mathcal{R}_i | \sigma) = p(\mathcal{R}_i) = \frac{1}{n}.
\end{align}



The key advantage of the \textit{cat} parameterisation is the removal of a term from the posterior density (\textbf{Eq \ref{eq:bayesian}}), or more accurately the replacement of a non-trivial $p(\vec{\mathcal{R}}^{\,} | \sigma)$ term with that of a uniform prior. 
This may facilitate efficient exploration of the posterior distribution by MCMC.




This parameterisation has been widely used in BEAST and BEAST 2 analyses \cite{drummond2006relaxed}. 
However, the recently developed constant distance operators  -- which are incompatible with the \textit{cat} parameterisation -- can yield an increase in mixing rate under \textit{real} by up to an order of magnitude over that of \textit{cat}, depending on the dataset \cite{zhang2020improving}. 






\subsubsection*{3. Quantiles}


Finally, rates can be parameterised as real numbers describing the rate's quantile with respect to some underlying clock model distribution. Under the  \textit{quant} parameterisation, each of the $2N-2$ elements in $\vec{\mathcal{R}}^{\,}$ are uniformly distributed.


\begin{align}
\vec{\mathcal{R}}^{\,} \in & \; \mathbb{R}^{2N-2}, 0 < \mathcal{R}_i < 1  \\
p(\mathcal{R}_i | \sigma) = & \; p(\mathcal{R}_i) = 1
\end{align}


Transforming these quantiles into rates invokes the i-CDF of the underlying \textit{real} clock model distribution.
Evaluation of the log-normal i-CDF is has high computational costs and
therefore an approximation is used instead.



\begin{align}
r(\mathcal{R}_i) =& \; \hat{F}^{-1}(\mathcal{R}_i)
\end{align}

\noindent
where $\hat{F}^{-1}$ is a linear piecewise approximation with 100 pieces.
While this approach has clear similarities with \textit{cat}, the domain of rates here is continuous instead of discrete.
In this project we extended the family of constant distance operators  \cite{zhang2020improving} so that they are compatible with \textit{quant}.
Further details on the \textit{quant} piecewise approximation and constant distance operators can be found in \textbf{\nameref{sect:S1Appendix}}. 








%A potential disadvantage of the \textit{quant} method would be the computational requirements of continuously evaluating the i-cdf.
%As the number of quantiles grows with the taxon count $N$, this drawback would be at its worst on large trees.
%Hence, rather than evaluating the exact i-cdf $F^{-1}$, an approximation $\hat{F}^{-1}$ will be used instead:


%\begin{align}
%r(\mathcal{R}_i) =& \; \hat{F}^{-1}(\mathcal{R}_i).
%\end{align}


%In this article we have extended \textit{quant} through a linear piecewise approximation of the i-cdf. As the piecewise approximation is linear, evaluating the derivatives $\frac{\partial}{\partial \mathcal{R}_i} \hat{F}^{-1}(\mathcal{R}_i) =  D \hat{F}^{-1}(\mathcal{R}_i)$ and $\frac{\partial}{\partial r_i} \hat{F}(r_i) = D \hat{F}(r_i)$  -- which are required for computing the Hastings ratios -- is trivial. The approximation is comprised of $n$ pieces (where $n$ is fixed) and upper and lower rate boundaries $r_\text{min}$ and $r_\text{max}$. The approximation is displayed in \textbf{Fig \ref{fig:rateparams}} and further detailed in \textbf{\nameref{sect:S1Appendix}}.


%Zhang and Drummond 2020 introduced several tree operators for the \textit{real} parameterisation -- including \texttt{Constant Distance}, \texttt{Simple Distance}, and \texttt{Small Pulley}. In this project, we extended these three operators so that they are compatible with the \textit{quant} parameterisation. These are presented in \textbf{\nameref{sect:S1Appendix}}. 


\clearpage
\subsection*{Clock model operators}
\label{sect:clockModelOperators}

The weight of an operator determines the probability of the operator being selected.
Weights are typically fixed throughout MCMC.
In BEAST 2, operators can have their own tunable parameter $s$, which determines the step size of the operator.
This term is learned over the course of the MCMC \cite{rosenthal2011optimal, bouckaert2019beast}. 
We define clock model operators as those which generate proposals for either $\vec{\mathcal{R}}^{\,}$ or $\sigma$.
Pre-existing BEAST 2 clock model operators are summarised in \textbf{Table \ref{table:kernels}}, and further operators are introduced throughout this paper.


\begin{table}[h!]
\centering
\begin{tabular}{l p{4.2cm} l l} 
 Operator & Description & Parameters & Parameterisations  \\
  \hline
 \texttt{RandomWalk} & Moves a single element by a tunable amount. & $\vec{\mathcal{R}}^{\,}, \sigma$ & \textit{cat}, \textit{real}, \textit{quant} \\
  \hline
\texttt{Scale} & Applies \texttt{RandomWalk} on the log-transformation (suitable for parameters with positive domains). & $\vec{\mathcal{R}}^{\,}, \sigma$ & \textit{cat}, \textit{real}, \textit{quant}  \\
  \hline
 \texttt{Interval} & Applies \texttt{RandomWalk} on the logit-transformation (suitable for parameters with upper and lower limits). & $\vec{\mathcal{R}}^{\,}$ & \textit{quant}  \\
  \hline
 \texttt{Swap} & Swaps two random elements within the vector \cite{drummond2006relaxed}. & $\vec{\mathcal{R}}^{\,}$  & \textit{cat}, \textit{real}, \textit{quant}  \\
 \hline
\texttt{Uniform} & Resamples one element in the vector from a uniform distribution. & $\vec{\mathcal{R}}^{\,}$  & \textit{cat}, \textit{quant}  \\
 \hline
\texttt{ConstantDistance} & Adjusts an internal node height and recalculates all incident branch rates such that the genetic distances remain constant \cite{zhang2020improving}.  & $\vec{\mathcal{R}}^{\,}, \mathcal{T}$ & \textit{real}, \textit{quant} \\
 \hline
\texttt{SimpleDistance} & Applies \texttt{ConstantDistance} to the root node \cite{zhang2020improving}.  & $\vec{\mathcal{R}}^{\,}, \mathcal{T}$ & \textit{real}, \textit{quant} \\
 \hline
\texttt{SmallPulley} & Proposes new branch rates incident to the root such that their combined genetic distance is constant  \cite{zhang2020improving}.  & $\vec{\mathcal{R}}^{\,}$ & \textit{real}, \textit{quant} \\
 \hline
\texttt{CisScale} & Applies \texttt{Scale} to $\sigma$. Then recomputes all rates such that their quantiles are constant (for \textit{real} \cite{zhang2020improving}) or recomputes all quantiles such that their rates are constant (\textit{quant}).  & $\vec{\mathcal{R}}^{\,}, \sigma$ & \textit{real}, \textit{quant} \\
\end{tabular}
\caption{Summary of pre-existing BEAST 2 operators, which apply to either branch rates $\vec{\mathcal{R}}^{\,}$ or the clock standard deviation $\sigma$, and the substitution rate parameterisation they apply to.
 \texttt{ConstantDistance} and \texttt{SimpleDistance} also adjust node heights in the tree $\mathcal{T}$. }
\label{table:kernels}
\end{table}





The family of constant distance operators (\texttt{ConstantDistance}, \texttt{SimpleDistance}, and \texttt{SmallPulley} \cite{zhang2020improving}) are best suited for larger datasets (or datasets with strong signal) where the likelihood distribution is peaked (\textbf{Fig \ref{fig:rateparams}}).
While simple one dimensional operators such as \texttt{RandomWalk} or \texttt{Scale} must take small steps in order to stay ``on the ridge'' of the likelihood function, the constant distance operators ``wander along the ridge''  by ensuring that genetic distances are constant after the proposal.



\begin{figure}[!h]
\includegraphics[width=\textwidth]{Figures/correlations.pdf}
\caption{\textbf{Traversing likelihood space.}
The z-axes above are the log-likelihoods of the genetic distance $r \times \tau$ between two simulated nucleic acid sequences of length $L$, under the Jukes-Cantor substitution model \cite{jukes1969evolution}. 
Two possible proposals from the current state (white circle) are depicted.
These proposals are generated by the \texttt{RandomWalk} (\texttt{RW}) and \texttt{ConstantDistance} (\texttt{CD}) operators.
In the low signal dataset ($L=0.1$kb), both operators can traverse the likelihood space effectively.
 However, the exact same proposal by \texttt{RandomWalk} incurs a much larger likelihood penalty in the $L=0.5$kb dataset by ``falling off the ridge'', in contrast to \texttt{ConstantDistance} which ``walks along the ridge''.
 This discrepancy is even stronger for larger datasets and thus necessitates the use of operators such as \texttt{ConstantDistance} which account for correlations between branch lengths and rates. }
\label{fig:landscape}
\end{figure}


 \texttt{Scale} and \texttt{CisScale} both operate on the clock model standard deviation $\sigma$ however they behave differently in the \textit{real} and \textit{quant} parameterisations (\textbf{Fig \ref{fig:clockSDoperators}}).
In \textit{real}, large proposals of $\sigma \rightarrow \sigma^\prime$ made by  \texttt{Scale} could incur large penalties in the clock model prior density $p(\vec{\mathcal{R}}^{\,}|\sigma^\prime)$ and thus may be rejected quite often. 
This led to the development of the fast clock scaler \cite{zhang2020improving} (herein referred to as \texttt{CisScale}).
This operator recomputes all branch rates $\vec{\mathcal{R}}^{\,} \rightarrow \vec{\mathcal{R}}^{\, \prime}$ such that their quantiles under the new clock model prior remain constant $p(\vec{\mathcal{R}}^{\,}|\sigma) = p(\vec{\mathcal{R}}^{\, \prime}|\sigma^\prime)$.
In contrast, a proposal made by \texttt{Scale} $\sigma \rightarrow \sigma^\prime$ under the \textit{quant} parameterisation implicitly alters all branch rates $r(\vec{\mathcal{R}}^{\,})$ while leaving the quantiles $\vec{\mathcal{R}}^{\,}$ themselves constant.
Whereas, application of \texttt{CisScale} under \textit{quant} results in all quantiles being recomputed $\vec{\mathcal{R}} \rightarrow \vec{\mathcal{R}}^\prime$  such that their rates are constant, i.e. $r(\vec{\mathcal{R}}^{\,}) = r(\vec{\mathcal{R}}^{\,\prime})$.
In summary, \texttt{Scale} and \texttt{CisScale} propose rates/quantiles in the opposite (trans) or same (cis) space that the clock model is parameterised under (\textbf{Fig \ref{fig:clockSDoperators}}).




\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth]{Figures/clockSD.pdf}
\caption{\textbf{Clock standard deviation scale operators.}
The two operators above propose a clock standard deviation $\sigma \rightarrow \sigma^\prime$. 
Then, either the new quantiles are such that the rates remain constant (``New quantiles'', above) or the new rates are  such that the quantiles remain constant (``New rates''). 
In the \textit{real} parameterisation, these two operators are known as \texttt{Scale} and \texttt{CisScale}, respectively.
Whereas, in \textit{quant}, they are known as \texttt{CisScale} and \texttt{Scale}.    }
\label{fig:clockSDoperators}
\end{figure}



\clearpage
\subsection*{Adaptive operator weighting}
\label{sect:adaptiveSampling}






It is not always clear which operator weighting scheme is best for a given dataset.
In this article we introduce \texttt{AdaptiveOperatorSampler} -- a meta-operator which learns the weights of other operators during MCMC and then samples these operators according to their learned weights.
This meta-operator undergoes three phases.
In the first phase (burn-in), \texttt{AdaptiveOperatorSampler} samples from its set of sub-operators uniformly at random.
In the second phase (learn-in), the meta-operator starts learning several terms detailed below whilst continuing to sample operators uniformly at random.
In its final phase, \texttt{AdaptiveOperatorSampler} samples operators (denoted by $\omega$) using the following distribution:


\begin{align}
\label{eqn:adaptiveSampler}
	p(\omega) \propto \begin{cases} 1 & \text{ with probability } \Omega \\ \frac{1}{\mathbb{T}(\omega)} \sum\limits_{p \in \text{POI}}  \sum\limits_{x \in \text{accepts}(\omega)}  \mathbb{D}(x_p, x_p^\prime) & \text{ with probability } 1-\Omega \end{cases}
\end{align}

\noindent
where $\mathbb{T}(\omega)$ is the cumulative computational time spent on each operator, $\mathbb{D}$ is a distance function, and we use $\Omega = 0.01$ to allow any sub-operator to be sampled regardless of its performance.
The parameters of interest (POI) may be either a set of numerical parameters (such as branch rates or node heights), or it may be the tree itself, but it cannot be both in its current form.
The distance between state $x_p$ and its (accepted) proposal $x_p^\prime$ with respect to parameter $p$ is determined by


\begin{align}
\label{eqn:distanceFunctions} 
	\mathbb{D}(x_p, x_p^\prime) = \begin{cases} \texttt{RF}(x_p, x_p^\prime)^2 & \text{ if } p \text{ is a tree} \\ 
	\frac{1}{|p|} \Big[ \frac{\norm{x_p - x_p^\prime}}{\sigma_p} \Big]^2    & \text{ if } p \text{ is numerical}  \end{cases}
\end{align}


\noindent
where \texttt{RF} is the Robinson-Foulds tree distance \cite{robinson1981comparison}, and $|p|$ is the number of dimensions of numerical parameter $p$ (1 for $\sigma$, $2N-2$ for $\vec{\mathcal{R}}^{\,}$, and $2N-1$ for node heights $t$).
The remaining terms are trained during the second and third phases: 
the sample standard deviation $\sigma_p$ of each numerical parameter of interest $p$,
the cumulative computational runtime spent on each operator $\mathbb{T}(\omega)$,  and the summed distances $\sum_x \mathbb{D}(x_p, x_p^\prime)$.



%The terms which are learned during the second and third phases are: , 





Under \textbf{Equations \ref{eqn:adaptiveSampler} and \ref{eqn:distanceFunctions}}, operators which effect larger changes on the parameters of interest, in shorter runtime, are sampled with greater probabilities. 
Division of the squared distance by a parameter's sample variance $\sigma^2_p$ enables comparison between numerical parameters which exist in different spaces.


Datasets which contain very poor signal (or small $L$) are likely to mix better when more weight is placed on bold operators (\textbf{Fig \ref{fig:landscape}}). 
We therefore introduce the \texttt{SampleFromPrior($\vec{x}^{\,}$)} operator.
This operator resamples $\psi$ randomly selected elements within vector $\vec{x}^{\,}$ from their prior distributions, where $\psi \sim \text{Binomial}(n=|\vec{x}^{\,}|, p=\frac{s}{|\vec{x}^{\,}|})$ for tunable term $s$.
%The \texttt{Uniform} operator is a special case of \texttt{SampleFromPrior}.
\texttt{SampleFromPrior} is included among the set of operators under \texttt{AdaptiveOperatorSampler} and serves to make the boldest proposals for datasets with poor signal.
 






In this article we apply three instances of the \texttt{AdaptiveOperatorSampler} meta-operator to the \textit{real}, \textit{cat}, and \textit{quant} parameterisations. 
These are summarised in \textbf{Table \ref{table:adaptiveSampling}}.



\begin{table}[h!]
\centering
\begin{tabular}{l l l} 
 Meta-operator & POI & Operators \\
\hline
 \multirow{4}{*}{\texttt{AdaptiveOperatorSampler($\sigma$)}} & \multirow{4}{*}{$\sigma$} & \texttt{CisScale($\sigma, \vec{\mathcal{R}}^{\,}$)} \\ 
 && \texttt{RandomWalk($\sigma$)}  \\
 && \texttt{Scale($\sigma$)}  \\
 && \texttt{SampleFromPrior($\sigma$)}  \\
 \hline
  \multirow{6}{*}{\texttt{AdaptiveOperatorSampler($\vec{\mathcal{R}}^{\,}$)}} & \multirow{6}{*}{$\vec{\mathcal{R}}^{\,}, t$} & \texttt{ConstantDistance($\vec{\mathcal{R}}^{\,}, \mathcal{T}$)}   \\ 
&& \texttt{RandomWalk($\vec{\mathcal{R}}^{\,}$)}  \\
&& \texttt{Scale($\vec{\mathcal{R}}^{\,}$)}   \\
&& \texttt{Interval($\vec{\mathcal{R}}^{\,}$)}   \\
&& \texttt{Swap($\vec{\mathcal{R}}^{\,}$)}  \\
&& \texttt{SampleFromPrior($\vec{\mathcal{R}}^{\,}$)} \\
 \hline
   \multirow{2}{*}{\texttt{AdaptiveOperatorSampler(root)}} & \multirow{2}{*}{$\vec{\mathcal{R}}^{\,}, t$} & \texttt{SimpleDistance($\vec{\mathcal{R}}^{\,}, \mathcal{T}$)}  \\ 
&&  \texttt{SmallPulley($\vec{\mathcal{R}}^{\,}, t$)}  \\
\end{tabular}
\caption{Summary of \texttt{AdaptiveOperatorSampler} operators and their parameters of interest (POI).
Different operators are applicable to different substitution rate parameterisations (\textbf{Table \ref{table:kernels}}). 
\texttt{AdaptiveOperatorSampler(root)} applies the root-targeting constant distance operators only \cite{zhang2020improving} while \texttt{AdaptiveOperatorSampler($\vec{\mathcal{R}}^{\,}$)} targets all rates and all nodes heights $t$. 
These two operators are weighted proportionally to the contribution of the root node to the total node count. }
\label{table:adaptiveSampling}
\end{table}

%RRB: should the root operator include a tree root scaler?



%\texttt{AdaptiveOperatorSampler($\sigma$)} samples the clock standard deviation, 
%\texttt{AdaptiveOperatorSampler($\mathcal{R}$)} samples branch rates and internal node heights,
%and \texttt{AdaptiveOperatorSampler(root)} samples  









\clearpage
\subsection*{Bactrian proposal kernel} \label{sect:randomwalks}


%The proposal kernel $q(x^\prime|x)$ defines the conditional probability of proposing state $x^\prime$ given state $x$. 

The step size of a proposal kernel $q(x^\prime|x)$ should be such that the proposed state $x^\prime$ is sufficiently far from the current state $x$ to explore vast areas of parameter space, but not so large that the proposal is rejected too often \cite{roberts1997weak}. 
Operators which attain an acceptance probability of 0.234 are often considered to have arrived at a suitable midpoint between these two extremes \cite{bouckaert2019beast, roberts1997weak}.
The standard uniform distribution kernel has recently been challenged by the family of Bactrian kernels \cite{yang2013searching, thawornwattana2018designing}.
The (Gaussian) $\text{Bactrian}(m)$ distribution is defined as the sum of two normal distributions:


\begin{align}
	\Sigma \sim \text{Bactrian}(m) \equiv \frac{1}{2}\text{Normal}(-m, 1-m^2) + \frac{1}{2}\text{Normal}(m, 1-m^2)
\end{align}


\noindent
where $0 \leq m < 1$ describes modality. 
When $m=0$, the Bactrian distribution is equivalent to $\text{Normal}(0, 1)$. 
As $m$ approaches 1, the distribution becomes increasingly bimodal (\textbf{Fig \ref{fig:bactrian}}). 
Yang et al. 2013 \cite{yang2013searching} demonstrate that $\text{Bactrian}(m=0.95)$ yields a proposal kernel which traverses the posterior distribution more efficiently that the standard uniform kernel, by placing minimal probability on steps which are too small or too large.
In this case, a target acceptance probability of around 0.3 is optimal.


\begin{figure}[!h]
\includegraphics[width=\textwidth]{Figures/bactrian.pdf}
\caption{\textbf{The Bactrian proposal kernel.} The step size made under a Bactrian proposal kernel is equal to $s \Sigma$ where $\Sigma$ is drawn from the above distribution and $s$ is tunable.   }
\label{fig:bactrian}
\end{figure}





In this article we compare the abilities of uniform and Bactrian(0.95) proposal kernels at estimating clock model parameters. 
The clock model operators which these proposal kernels apply to are described in \textbf{Table \ref{table:bactriankernels}}.





\begin{table}[h!]
\centering
\begin{tabular}{l p{3cm} l l} 

 & Operator(s) & Proposal & Parameter $x$   \\
   \hline
 1 & \texttt{RandomWalk}  & $x^\prime \leftarrow x + s\Sigma$ & $\vec{\mathcal{R}}^{\,}, \sigma$  \\
  \hline
 2 & \texttt{Scale} & $x^\prime \leftarrow x \times e^{s\Sigma}$ & $\vec{\mathcal{R}}^{\,}, \sigma$   \\
  \hline
 3 & \texttt{Interval} & $\begin{array} {rl} &y \leftarrow \frac{1 - x}{x} \times e^{s\Sigma} \\ &x^\prime \leftarrow \frac{y}{y + 1}  \end{array}$ & $\vec{\mathcal{R}}^{\,}$  \\
  \hline
 4 & \texttt{ConstantDistance} \texttt{SimpleDistance} & $x^\prime \leftarrow x + s\Sigma$ & $t$ \\
 \hline
 5 & \texttt{SmallPulley} & $x^\prime \leftarrow x + s\Sigma$ & $\vec{\mathcal{R}}^{\,}$  \\
 \hline
6 & \texttt{CisScale}  & $x^\prime \leftarrow x \times e^{s\Sigma}$ & $\sigma$   \\
\end{tabular}
\caption{Proposal kernels $q(x^\prime|x)$ of clock model operators.
 In each operator, $\Sigma$ is drawn from either a $\text{Bactrian}(m)$ or $\text{uniform}$ distribution.  %RB: Uniform on which interval? (-1,1)?
 The scale size $s$ is tunable.
 \texttt{ConstantDistance} and \texttt{SimpleDistance} propose tree heights $t$.
  The \texttt{Interval} operator applies to rate quantiles and respects its domain i.e. $0 < x, x^\prime < 1$. }
\label{table:bactriankernels}
\end{table}

% RB: Perhaps add Hastings ratios to the table?



% Give the equation of the Uniform + Bactrian kernels
% Fig: Bactrian m=0, 0.92, 0.95, 0.98, + Uniform(-1,1)
% In this article we compare the Uniform + Bactrian proposals in the clock model  

% BRIEFLY describe three operators: random walk, scale, uniform + where the proposal kernel comes in + which parameters in the clock model it applies to



% $x^\prime \leftarrow w \times \frac{u - x}{x - l}$


\clearpage
% REMEMBER to define NNI and SPR acronyms in introduction preferably
\subsection*{Narrow Exchange Rate} \label{sect:NER}

The \texttt{NarrowExchange} operator \cite{drummond2002estimating}, used widely in BEAST \cite{drummond2012bayesian,suchard2018bayesian} and BEAST 2 \cite{bouckaert2019beast}, is similar to nearest neighbour interchange \cite{semple2003phylogenetics}, and works as follows (\textbf{Fig \ref{fig:narrowexchange}}):

\textul{\textit{Step 1}}. Sample an internal/root node $E$ from tree $\mathcal{T}$, where $E$ has grandchildren.

\textul{\textit{Step 2}}. Identify the child of $E$ with the greater height. Denote this child as $D$ and its sibling as $C$ (i.e. $t_D > t_C$). If $D$ is a leaf node, then reject the proposal.

\textul{\textit{Step 3}}. Randomly identify the two children of $D$ as $A$ and $B$.

\textul{\textit{Step 4}}. Relocate the $B-D$ branch onto the $C-E$ branch, so that $B$ and $C$ become siblings and their parent is $D$. All node heights remain constant.


\begin{figure}[!h]
\includegraphics[width=\textwidth]{Figures/NarrowExchange.pdf}
\caption{\textbf{Depiction of \texttt{NarrowExchange} and \texttt{NarrowExchangeRate} operators.} Proposals are denoted by $\mathcal{T} \rightarrow \mathcal{T}^\prime$. The vertical axes correspond to node heights $t$. In the bottom figure, branch rates $r$ are indicated by line width and therefore genetic distances are equal to the width of each branch multiplied by its length. 
In this example, the $\mathcal{D}_{AE}$ and $\mathcal{D}_{CE}$ constraints are satisfied.
%RB: at this stage, D_AE and D_CE constraints have not been defined yet. Remove or reformulate or move to after definition?
}
\label{fig:narrowexchange}
\end{figure}



We hypothesised that if \texttt{NarrowExchange} was adapted to the relaxed clock model by ensuring that genetic distances remain constant after the proposal (analogous to constant distance operators \cite{zhang2020improving}), then its ability to traverse the state space may improve. 


%Therefore, it seems reasonable to adapt the Narrow Exchange operator for use with the relaxed clock model, such that genetic distances remain constant after performing the proposal $\mathcal{T} \rightarrow \mathcal{T}^\prime$.


Here, we present the \texttt{NarrowExchangeRate} (NER) operator. 
Let $r_A$, $r_B$, $r_C$, and $r_D$ be the substitution rates of nodes $A$, $B$, $C$, and $D$, respectively. 
In addition to the modest topological change applied by \texttt{NarrowExchange}, NER also proposes new branch rates ${r_A}^\prime$, ${r_B}^\prime$, ${r_C}^\prime$, and ${r_D}^\prime$. While NER does not alter $t_D$ (i.e. ${t_D}^\prime \leftarrow t_D$), we also consider NERw -- a special case of the NER operator which embarks $t_D$ on a random walk:

\begin{align}
	{t_D}^\prime \leftarrow t_D + s\Sigma
\end{align}

\noindent
for random walk step size $s\Sigma$ where $s$ is tunable and $\Sigma$ is drawn from a uniform or Bactrian distribution. NER (and NERw) are compatible with both the \textit{real} and \textit{quant} parameterisations. 
Analogous to the \texttt{ConstantDistance} operator, the proposed rates ensure that the genetic distances between nodes $A$, $B$, $C$, and $E$ are constant. 
Let $\mathcal{D}_{ij}$ be the constraint defined by a constant genetic distance between nodes $i$ and $j$ before and after the proposal.
There are six pairwise distances between these four nodes and therefore there are six such constraints:


\begin{align}
	\mathcal{D}_{AB}: \quad & r_A  (t_D - t_A) + r_B  (t_D - t_B) = \nonumber \\
					 & {r_A}^\prime  (t_E - t_A) + {r_D}^\prime  (t_E - {t_D}^\prime) + {r_B}^\prime ({t_D}^\prime - t_B) \\
	\mathcal{D}_{AC}: \quad & r_A  (t_D - t_A) + r_D  (t_E - t_D) + r_C  (t_E - t_C) = \nonumber \\
				 	  & {r_A}^\prime  (t_E - t_A) + {r_D}^\prime  (t_E - {t_D}^\prime) + {r_C}^\prime ({t_D}^\prime - t_C) \\
 	\mathcal{D}_{AE}: \quad & r_A  (t_D - t_A) + r_D  (t_E - t_D)= \nonumber \\
					  & {r_A}^\prime  (t_E - t_A) \\
  	\mathcal{D}_{BC}: \quad & r_B  (t_D - t_B) + r_D  (t_E - t_D) + r_C  (t_E - t_D)= \nonumber \\
					  & {r_B}^\prime ({t_D}^\prime - t_B) + {r_C}^\prime ({t_D}^\prime - t_C) \\
   	\mathcal{D}_{BE}: \quad & r_B  (t_D - t_B) + r_D  (t_E - t_D)= \nonumber \\
					  & {r_B}^\prime ({t_D}^\prime - t_B) + {r_D}^\prime (t_E - {t_D}^\prime) \\
	\mathcal{D}_{CE}: \quad & r_C  (t_E - t_C)= \nonumber \\
					  & {r_C}^\prime ({t_D}^\prime - t_C) + {r_D}^\prime (t_E - {t_D}^\prime) 
\end{align}


Further constraints are imposed by the model itself:


\begin{align}
	r_i, {r_i}^\prime > 0 \text { for } i \in \{A,B,C,D\} \\
	t_B, t_C  < {t_D}^\prime < t_E.
\end{align}



Unfortunately, there is no solution to all six $\mathcal{D}_{ij}$ constraints unless non-positive rates or illegal trees are permitted.
 Therefore instead of conserving all six pairwise distances, NER conserves a subset of distances. It is not immediately clear which subset should be conserved. 


\subsubsection*{Automated generation of operators and constraint satisfaction}


The total space of NER operators consists of all possible subsets of distance constraints (i.e. $\{\},\{\mathcal{D}_{AB}\}, \{\mathcal{D}_{AC}\}, \dotso , \{\mathcal{D}_{AB}, \mathcal{D}_{AC}, \mathcal{D}_{AE}, \mathcal{D}_{BC}, \mathcal{D}_{BE}, \mathcal{D}_{CE} \}$) that are solvable. 
The simplest NER kernel -- the null operator denoted by NER$\{\}$ -- does not satisfy any distance constraints and is equivalent to \texttt{NarrowExchange}. 
To determine which NER variants have the best performance, we developed an automated pipeline for generating and testing these operators.


\paragraph{1. Solution finding.} Using standard analytical linear-system solving libraries in MATLAB \cite{higham2016matlab}, the $2^6=64$ subsets of distance constraints were solved. 54 of the 64 subsets were found to be solvable, and the unsolvables were discarded.


\paragraph{2. Solving Jacobian determinants.} The determinant of the Jacobian matrix $J$ is required for computing the Green ratio of this proposal. $J$ is defined as 


\begin{align}
	J &= \begin{bmatrix} \frac{\partial {r_A}^\prime}{\partial r_A} & \frac{\partial {r_A}^\prime}{\partial r_B} & \frac{\partial {r_A}^\prime}{\partial r_C} & \frac{\partial {r_A}^\prime}{\partial r_D} & \frac{\partial {r_A}^\prime}{\partial t_D} \\
	\frac{\partial {r_B}^\prime}{\partial r_A} & \frac{\partial {r_B}^\prime}{\partial r_B} & \frac{\partial {r_B}^\prime}{\partial r_C} & \frac{\partial {r_B}^\prime}{\partial r_D} & \frac{\partial {r_B}^\prime}{\partial t_D} \\
	\frac{\partial {r_C}^\prime}{\partial r_A} & \frac{\partial {r_C}^\prime}{\partial r_B} & \frac{\partial {r_C}^\prime}{\partial r_C} & \frac{\partial {r_C}^\prime}{\partial r_D} & \frac{\partial {r_C}^\prime}{\partial t_D} \\
	\frac{\partial {r_D}^\prime}{\partial r_A} & \frac{\partial {r_D}^\prime}{\partial r_B} & \frac{\partial {r_D}^\prime}{\partial r_C} & \frac{\partial {r_D}^\prime}{\partial r_D} & \frac{\partial {r_D}^\prime}{\partial t_D} \\
\frac{\partial {t_D}^\prime}{\partial r_A} & \frac{\partial {t_D}^\prime}{\partial r_B} & \frac{\partial {t_D}^\prime}{\partial r_C} & \frac{\partial {t_D}^\prime}{\partial r_D} & \frac{\partial {t_D}^\prime}{\partial t_D}\end{bmatrix}.  \nonumber  \\
\end{align}


Solving the determinant $|J|$ invokes standard analytical differentiation and linear algebra libraries of MATLAB. 
6 of the 54 solvable operators were found to have $|J|=0$, corresponding to irreversible proposals, and were discarded. 


\paragraph{3. Automated generation of BEAST 2 operators.} Java class files were generated using string processing. 
Each class corresponded to a single operator, extended the class of a meta-NER-operator, and consisted of the solutions found in \textbf{1} and the Jacobian determinant found in \textbf{2}. 
$|J|$ is further augmented if the \textit{quant} parameterisation is employed (\textbf{\nameref{sect:S1Appendix}}).
Two such operators are expressed in \textbf{Algorithms \ref{alg:NER1} and \ref{alg:NER2}}.
%RB: what is the purpose of having 2 algorithms -- one (the second one, since that is the one performing well) is enough, no?



\begin{algorithm}
\caption{The NER$\{ \mathcal{D}_{BC}, \mathcal{D}_{CE} \}$ operator.}
\begin{algorithmic}[1]

\Procedure{proposal}{$t_A$, $t_B$, $t_C$, $t_D$, $t_E$, $r_A$, $r_B$, $r_C$, $r_D$}     

	\State
    \State $s\Sigma \leftarrow $ getRandomWalkSize() \Comment{Random walk size is 0 unless this is NERw}
    \State $t_D^\prime \leftarrow t_D + s\Sigma$ \Comment{Propose new node height for $D$}
    \State
    \State $r_A^\prime \leftarrow r_A$ \Comment{Propose new rates}
    \State $r_B^\prime \leftarrow \frac{r_B(t_D - t_B) + r_D(t_E - t_D) + r_D(t_E - t_D^\prime)}{t_D^\prime - t_B}$
    \State $r_C^\prime \leftarrow \frac{r_C(t_E - t_C) - r_D(t_E - t_D^\prime)}{t_D^\prime - t_C}$
    \State $r_D^\prime \leftarrow r_D$
    \State
    \State $|J| \leftarrow \frac{(t_D - t_B)(t_E - t_C)}{(t_D^\prime - t_B)(t_D^\prime - t_C)}$ \Comment{Calculate Jacobian determinant}
    \State \Return $(r_A^\prime, r_B^\prime, r_C^\prime, r_D^\prime, t_D^\prime, |J|)$
    
\EndProcedure

\end{algorithmic}
\label{alg:NER1}
\end{algorithm}


\begin{algorithm}
\caption{The NER$\{ \mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE} \}$ operator.}
\begin{algorithmic}[1]

\Procedure{proposal}{$t_A$, $t_B$, $t_C$, $t_D$, $t_E$, $r_A$, $r_B$, $r_C$, $r_D$}     

	\State
    \State $s\Sigma \leftarrow $ getRandomWalkSize() \Comment{Random walk size is 0 unless this is NERw}
    \State $t_D^\prime \leftarrow t_D + s\Sigma$ \Comment{Propose new node height for $D$}
    \State
    \State $r_A^\prime \leftarrow \frac{r_A(t_D - t_A) + r_D(t_E - t_D)}{t_E - t_A}$ \Comment{Propose new rates}
    \State $r_B^\prime \leftarrow \frac{r_B(t_D - t_B) + r_D(t_D^\prime - t_D)}{t_D^\prime - t_B}$
    \State $r_C^\prime \leftarrow \frac{r_C(t_E - t_C) - r_D(t_E - t_D^\prime)}{t_D^\prime - t_C}$
    \State $r_D^\prime \leftarrow r_D$
    \State
    \State $|J| \leftarrow \frac{(t_D - t_A)(t_D - t_B)(t_E - t_C)}{(t_E - t_A)(t_D^\prime - t_B)(t_D^\prime - t_C)}$ \Comment{Calculate Jacobian determinant}
    \State \Return $(r_A^\prime, r_B^\prime, r_C^\prime, r_D^\prime, t_D^\prime, |J|)$
    
\EndProcedure

\end{algorithmic}
\label{alg:NER2}
\end{algorithm}





\paragraph{4. Screening operators for acceptance rate using simulated data.}


Selecting the best NER variant to proceed to benchmarking on empirical data (\textbf{\nameref{sect:results}}) was determined by performing MCMC on simulated data, measuring the acceptance rates of each of the 96 NER/NERw variants, and comparing them with the null operator $\text{NER}\{\}$ / \texttt{NarrowExchange}. 
In total, there were 300 simulated datasets each with $N=30$ taxa and varying alignment lengths.


%The acceptance rate of each operator (which in this case is equivalent to the average \texttt{RF} distance) was compared to that of the null operator $\text{NER}\{\}$ (i.e. \texttt{NarrowExchange}). 

These experiments showed that NER variants which satisfied the genetic distances between nodes $B$ and $A$ (i.e. $\mathcal{D}_{AB}$) or between $B$ and $C$ (i.e. $\mathcal{D}_{BC}$) usually performed worse than the standard \texttt{NarrowExchange} operator (\textbf{Fig \ref{fig:acceptanceRateScreening}}).
This is an intuitive result. 
If there is high uncertainty in the positioning of $B$ with respect to $A$ and $C$, then there is no value in respecting either of these distance constraints, and the proposals made to the rates may often be too extreme or the Green ratio $|J|$ too small for the proposal to be accepted.



\begin{figure}[!h]
\includegraphics[width=\textwidth]{Figures/acceptanceRates.pdf}
\caption{\textbf{Screening of NER and NERw variants by acceptance rate.} Top left: comparison of NER variants with the null operator $\text{NER}\{\}$ = \texttt{NarrowExchange}. 
Each operator is represented by a single point, uniquely encoded by the point stylings. 
The number of times each operator is proposed and accepted is compared with that of $\text{NER}\{\}$, and one-sided z-tests are performed to assess the statistical significance between the two acceptance rates ($p = 0.001$).  This process is repeated across $300$ simulated datasets. The axes of each plot are the proportion of these $300$ simulations for which there is evidence that the operator is significantly better than $\text{NER}\{\}$ (x-axis) or worse than $\text{NER}\{\}$ (y-axis). Top right: comparison of NER and NERw acceptance rates. Each point is one NER/NERw variant from a single simulation. Bottom: relationship between the acceptance rates $\alpha$ of $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$ and $\text{NER}\{ \}$ with the clock model standard deviation $\sigma$ and the number of sites $L$.  Each point is a single simulation. }
\label{fig:acceptanceRateScreening}
\end{figure}





\textbf{Fig \ref{fig:acceptanceRateScreening}} also revealed a cluster of NER variants which -- under the conditions of the simulation --  performed better than the null operator $\text{NER}\{\}$ around 25\% of the time and performed worse around 10\% of the time. 
One such operator was  $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$  (\textbf{Algorithm \ref{alg:NER2}}). 
This variant conserves the genetic distance between nodes $A$, $B$, $C$ and their grandparent $E$. 
This operator performed well when branch rates had a large variance ($\sigma > 0.5$), corresponding to non clock-like data. 
On the other hand, the null operator $\text{NER}\{\}$ performed better on shorter sequences ($L < 1$kb) with weaker signal.  
Overall, $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$ outperformed the standard \texttt{NarrowExchange} operator when the data was not clock-like and contained sufficient signal. 



Finally, this initial screening showed that applying a (Bactrian) random walk to the node height $t_D$ made the operator worse.
This effect was most dominant for the NER variants which satisfied distance constraints (i.e. the operators which are not $\text{NER}\{\}$).




Although there were several operators which behaved equivalently during this initial screening process, we selected $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$ to proceed to benchmarking (\textbf{\nameref{sect:results}}).
Due to the apparent sensitivity of NER operators to the data, we introduce the adaptive operator \texttt{AdaptiveOperatorSampler(NER)} which allows the operator scheme to fall back on the standard \texttt{NarrowExchange} in the event of $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$ performing poorly (\textbf{Table \ref{table:adaptiveNER}}).





\begin{table}[h!]
\centering
\begin{tabular}{l l l} 
 Meta-operator & POI & Operators \\
\hline
 \multirow{2}{*}{\texttt{AdaptiveOperatorSampler(NER)}} & \multirow{2}{*}{$\mathcal{T}$} & NER$\{  \}$ \\ 
 && NER$\{ \mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE} \}$ \\
\end{tabular}
\caption{The adaptive NER operator. 
The Robinson-Foulds distance between trees before and after every proposal accept is used to train the operator weights. In the special case of NER proposals, the \texttt{RF} distance is always equal to 1. }
\label{table:adaptiveNER}
\end{table}






\clearpage
\subsection*{An adaptive leaf rate operator}
\label{AVMVN_sect}

%, such as \texttt{AdaptiveOperatorSampler},
The adaptable variance multivariate normal (AVMVN) kernel  learns correlations between parameters during MCMC \cite{baele2017adaptive,suchard2018bayesian}. 
Baele et al. 2017  observed a large increase ($\approx 5-10 \times$) in sampling efficiency from using the AVMVN kernel substitution model parameters \cite{baele2017adaptive}.  
Here, we consider application of the AVMVN kernel to the branch rates of leaf nodes. 
This operator, referred to as \texttt{LeafAVMVN}, is not readily applicable to internal node branch rates due to their dependencies on tree topology.  




\subsubsection*{Leaf rate AVMVN kernel}


The AVMVN kernel assumes its parameters live in $x \in \mathbb{R}^N$ for taxon count $N$ and that these parameters follow a multivariate normal distribution with covariance matrix $\Sigma_N$. Hence, the kernel operates on the logarithmic or logistic transformation of the $N$ leaf branch rates, depending on the rate parameterisation:

\begin{align}
	x = \begin{cases} \log r \text{ for } \textit{real} \\
						\log \frac{q}{1 - q} \text{ for } \textit{quant}  \end{cases}
\end{align}

\noindent
where $r$ is a real rate and $q$ is a rate quantile. The AVMVN probability density is defined by 


\begin{align}
	\mathcal{AVMVN}(\vec{x}) =  \mathcal{MVN}\big(\vec{x}, (1-\beta) \frac{\Sigma_N}{N} + \beta \frac{\mathbb{I}_N}{N} \big) ,
\end{align}


\noindent
where $\mathcal{MVN}$ is the multivariate normal probability density. $\beta  = 0.05$ is a constant which determines the fraction of the proposal determined by the identity matrix $\mathbb{I}_N$, as opposed to the covariance matrix $\Sigma_D$ which is trained during MCMC.
Our BEAST 2 implementation of the AVMVN kernel is adapted from that of BEAST \cite{suchard2018bayesian}.

%The AVMVN proposal kernel is computed as


%\begin{align}
%	x^\prime &\leftarrow x + \sum\limits_{i=1}^N \sum\limits_{j=i}^N c_{i,j} \times s\Sigma \\
%	\text {where }  c &= \text{cholesky} \left( (1-\beta) \frac{\Sigma_N}{N} + \beta \frac{\mathbb{I}_N}{N} \right).
%\end{align}


%The cholesky$(Y)$ decomposition returns a lower diagonal matrix $L$, with positive real diagonal entries, such that $Y = LL^\prime$ \cite{lindstrom1988newton, pourahmadi2007cholesky}. $s$ is a tunable step size parameter and $\Sigma$ is a random variable drawn from a proposal kernel (uniform or Bactrian for instance). 


\texttt{LeafAVMVN} has the advantage of operating on all $N$ leaf rates simultaneously (as well as learning their correlations), as opposed to \texttt{ConstantDistance} which operates on at most 2, or \texttt{Scale} which operates on at most 1 leaf rate at a time. 
As the size of the covariance matrix $\Sigma_N$ grows with the number of taxa $N$, \texttt{LeafAVMVN} is likely to be less efficient with larger taxon sets.
Therefore, the weight behind this operator is learned by \texttt{AdaptiveOperatorSampler}.



To prevent the learned weight behind \texttt{LeafAVMVN} from dominating the \texttt{AdaptiveOperatorSampler} weighting scheme and therefore inhibiting the mixing of internal node rates, we 
introduce the \texttt{AdaptiveOperatorSampler(leaf)} and \texttt{AdaptiveOperatorSampler(internal)} meta-operators which operate exclusively on leaf node rates $\vec{\mathcal{R}}^{\,}_\text{leaf}$ and internal node rates $\vec{\mathcal{R}}^{\,}_\text{int}$ respectively  (\textbf{Table \ref{table:AVMVNoperators}}). 
The former employs the \texttt{LeafAVMVN} operator and learns its weight during MCMC (after providing it sufficient time to learn $\Sigma_N$).



\begin{table}[h!]
\centering
\begin{tabular}{l l l} 
 Meta-operator & POI & Operators \\
\hline
 \multirow{7}{*}{\texttt{AdaptiveOperatorSampler(leaf)}} & \multirow{7}{*}{$\vec{\mathcal{R}}^{\,}_\text{leaf}, t$} & \texttt{ConstantDistance($\vec{\mathcal{R}}^{\,}_\text{leaf}, \mathcal{T}$)}   \\ 
&& \texttt{LeafAVMVN($\vec{\mathcal{R}}^{\,}_\text{leaf}$)}  \\
&& \texttt{RandomWalk($\vec{\mathcal{R}}^{\,}_\text{leaf}$)}  \\
&& \texttt{Scale($\vec{\mathcal{R}}^{\,}_\text{leaf}$)}   \\
&& \texttt{Interval($\vec{\mathcal{R}}^{\,}_\text{leaf}$)}   \\
&& \texttt{Swap($\vec{\mathcal{R}}^{\,}_\text{leaf}$)}  \\
&& \texttt{SampleFromPrior($\vec{\mathcal{R}}^{\,}_\text{leaf}$)} \\
\hline
 \multirow{6}{*}{\texttt{AdaptiveOperatorSampler(internal)}} & \multirow{6}{*}{$\vec{\mathcal{R}}^{\,}_\text{int}, t$} & \texttt{ConstantDistance($\vec{\mathcal{R}}^{\,}_\text{int}, \mathcal{T}$)}   \\ 
&& \texttt{RandomWalk($\vec{\mathcal{R}}^{\,}_\text{int}$)}  \\
&& \texttt{Scale($\vec{\mathcal{R}}^{\,}_\text{int}$)}   \\
&& \texttt{Interval($\vec{\mathcal{R}}^{\,}_\text{int}$)}   \\
&& \texttt{Swap($\vec{\mathcal{R}}^{\,}_\text{int}$)}  \\
&& \texttt{SampleFromPrior($\vec{\mathcal{R}}^{\,}_\text{int}$)} \\
\end{tabular}
\caption{Leaf rate $\vec{\mathcal{R}}^{\,}_\text{leaf}$ and internal node rate $\vec{\mathcal{R}}^{\,}_\text{int}$ operators. 
This division enables the two meta-operators to be weighted proportionally to the number of nodes (leaves or internal) which they apply to.
This facilitates incorporation of the  \texttt{LeafAVMVN} operator, which is only applicable to leaf nodes.
In this setup, the \texttt{RandomWalk($x$)}, \texttt{Scale($x$)}, and \texttt{SampleFromPrior($x$)} operators apply to the corresponding set of branch rates $x$, whereas \texttt{ConstantDistance($x, \mathcal{T}$)} is only applicable to internal nodes which have at least one child of type $x \in \{\vec{\mathcal{R}}^{\,}_\text{leaf}, \vec{\mathcal{R}}^{\,}_\text{int}\}$.}
\label{table:AVMVNoperators}
\end{table}








\clearpage
\subsection*{Model specification and MCMC settings} \label{sect:methods}

In all phylogenetic analyses presented here, we use a Yule \cite{yule1925ii} tree prior $p(\mathcal{T}|\lambda)$ with birth rate $\lambda \sim \text{Log-normal}(1,1.25)$.
Here and throughout the article, a $\text{Log-normal}(a,b)$ distribution is parameterised such that $a$ and $b$ are the mean and standard deviation in log-space.
The clock standard deviation has a $\sigma \sim \text{Gamma}(0.5396,0.3819)$ prior.
Datasets are partitioned into subsequences, where each partition is associated with a distinct HKY substitution model \cite{hasegawa1985dating}.
The transition-transversion ratio $\kappa \sim \text{Log-normal}(1, 1.25)$, the four nucleotide frequencies $(f_A, f_C, f_G, f_T) \sim \text{Dirichlet}(10,10,10,10)$, and the relative clock rate $\mu_C \sim \text{Log-normal}(-0.18, 0.6)$ are estimated independently for each partition.
The operator scheme ensures that the clock rates $\mu_C$ have a mean of 1 across all partitions. 
This avoids non-identifiability with branch substitution rates.
To enable rapid benchmarking of larger datasets we use BEAGLE for high-performance tree likelihood calculations \cite{ayres2012beagle} and coupled MCMC with four chains for efficient mixing \cite{muller2019coupled}.
The neighbour joining tree \cite{saitou1987neighbor} is used as the initial state in each MCMC chain.



Throughout the article, we have introduced four new operators. 
These are summarised in \textbf{Table \ref{table:newOperators}}.


\begin{table}[h!]
\centering
\begin{tabular}{l p{4cm} l} 
 Operator & Description & Parameters  \\
  \hline
 \texttt{AdaptiveOperatorSampler} & Samples sub-operators proportionally to their weights, which are learned (see \nameref{sect:adaptiveSampling}). & $\vec{\mathcal{R}}^{\,}, \sigma, \mathcal{T}$ \\
  \hline
 \texttt{SampleFromPrior} & Resamples a random number of elements from their prior (see \nameref{sect:adaptiveSampling}). & $\vec{\mathcal{R}}^{\,}, \sigma$ \\
  \hline
 \texttt{NarrowExchangeRate} & Moves a branch and recomputes branch rates so that their genetic distances are constant (see \nameref{sect:NER}). & $\vec{\mathcal{R}}^{\,}, \mathcal{T}$\\
  \hline
 \texttt{LeafAVMVN}  & Proposes new rates for all leaves in one move (see \nameref{AVMVN_sect}) \cite{baele2017adaptive}. & $\vec{\mathcal{R}}^{\,}$ \\
\end{tabular}
\caption{Summary of clock model operators introduced throughout this article. Pre-existing clock model operators are summarised in \textbf{Table \ref{table:kernels}}}
\label{table:newOperators}
\end{table}


In \textbf{Table \ref{table:operatorSchemes}}, we define all operator configurations which are benchmarked throughout \textbf{\nameref{sect:results}}.



\begin{table}[h!]
\centering
\begin{tabular}{l l l l l l} 
Configuration & Operator & Weight & \textit{real} & \textit{cat} & \textit{quant}  \\
  \hline
 \multirow{6}{*}{nocons} & \texttt{RandomWalk($\vec{\mathcal{R}}^{\,}$)} & $10$ & \checkmark & \checkmark &  \\
 	& \texttt{Scale($\vec{\mathcal{R}}^{\,}$)} & $10$ & \checkmark && \\ 
 	& \texttt{Uniform($\vec{\mathcal{R}}^{\,}$)} & $10$ & & \checkmark & \checkmark \\ 
 	& \texttt{Interval($\vec{\mathcal{R}}^{\,}$)} & $10$ &  & & \checkmark \\ 
 	& \texttt{Swap($\vec{\mathcal{R}}^{\,}$)} & $10$ & \checkmark & \checkmark & \checkmark \\ 
 	& \texttt{Scale($\sigma$)} & $10$ & \checkmark & \checkmark & \checkmark \\ 
  \hline
   \multirow{10}{*}{cons} 
   & \texttt{ConstantDistance($\vec{\mathcal{R}}^{\,}, \mathcal{T}$)} & $20 \times \frac{2N-2}{2N-1}$ & \checkmark & & \checkmark  \\
   & \texttt{SimpleDistance($\vec{\mathcal{R}}^{\,}, \mathcal{T}$)} & $\frac{20}{2} \times \frac{1}{2N-1}$ & \checkmark & & \checkmark  \\
   & \texttt{SmallPulley($\vec{\mathcal{R}}^{\,}$)} & $\frac{20}{2} \times \frac{1}{2N-1}$ & \checkmark & & \checkmark  \\
   & \texttt{RandomWalk($\vec{\mathcal{R}}^{\,}$)} & $5$ & \checkmark &&  \\
 	& \texttt{Scale($\vec{\mathcal{R}}^{\,}$)} & $2.5$ & \checkmark && \\ 
 	& \texttt{Uniform($\vec{\mathcal{R}}^{\,}$)} & $5$ & &&\checkmark \\ 
 	& \texttt{Interval($\vec{\mathcal{R}}^{\,}$)} & $2.5$ & &&\checkmark\\ 
 	& \texttt{Swap($\vec{\mathcal{R}}^{\,}$)} & $2.5$ & \checkmark & & \checkmark  \\ 
 	& \texttt{CisScale($\sigma, \vec{\mathcal{R}}^{\,}$)} & $10$ & \checkmark  &&  \\ 
 	& \texttt{Scale($\sigma$)} & $10$ &  &&\checkmark \\ 
  \hline
     \multirow{3}{*}{adapt} & \texttt{AdaptiveOperatorSampler($\sigma$)} & $10$ & \checkmark & \checkmark & \checkmark  \\
 	& \texttt{AdaptiveOperatorSampler($\vec{\mathcal{R}}^{\,}$)} & $30 \times \frac{2N-2}{2N-1}$ & \checkmark & & \checkmark  \\
 	& \texttt{AdaptiveOperatorSampler($\vec{\mathcal{R}}^{\,}$)} & $30$ &  & \checkmark & \\
 	& \texttt{AdaptiveOperatorSampler(root)} & $30 \times \frac{1}{2N-1}$ & \checkmark & & \checkmark  \\
 	 \hline
 	 \multirow{4}{*}{AVMVN} & \texttt{AdaptiveOperatorSampler($\sigma$)} & $10$ & \checkmark &  & \checkmark  \\
 	& \texttt{AdaptiveOperatorSampler(leaf)} & $30 \times \frac{N}{2N-1}$ & \checkmark & & \checkmark  \\
 	& \texttt{AdaptiveOperatorSampler(internal)} & $30 \times \frac{N-2}{2N-1}$ &  \checkmark &  & \checkmark \\
 	 & \texttt{AdaptiveOperatorSampler(root)} & $30 \times \frac{1}{2N-1}$ & \checkmark & & \checkmark  \\
 	 \hline
 	 \hline
 	\multirow{1}{*}{NER\{\}} & \texttt{NarrowExchange} & $15$ & \checkmark & \checkmark  & \checkmark  \\
 	\hline
 	\multirow{1}{*}{NER} & \texttt{AdaptiveOperatorSampler(NER)} & $15$ & \checkmark &  & \checkmark  \\
\end{tabular}
\caption{Operator configurations and the substitution rate parameterisations which each operator is applicable to. Within each configuration (and substitution rate parameterisation), the weight behind $\vec{\mathcal{R}}^{\,}$ sums to 30, the weight of $\sigma$ is equal to 10, and the weight of NER is equal to 15.
%There is no cons (\textit{cat}) configuration because the parameterisation is not compatible the constant distance operators.
Operators which apply to specific node sets (root, internal, leaf, or all)
 are weighted according to leaf count $N$. The adaptive operators are further broken down in \textbf{Tables \ref{table:adaptiveSampling}, \ref{table:adaptiveNER}, and \ref{table:AVMVNoperators}}.
All other operators (i.e. those which apply to which apply to other terms in the state such as the nucleotide substitution model) are held constant within each dataset.
} 

%The total weight of all operators across all parameters was 116 in each MCMC setup.
%%herefore a weight of 10 corresponds to sampling probability of 0.086, for example.
\label{table:operatorSchemes}
\end{table}





\clearpage
\section*{Results} \label{sect:results}





To avoid a cross-product explosion, the five targets for clock model improvement were evaluated sequentially in the following order: \textbf{\nameref{sect:adaptiveSampling}}, \textbf{\nameref{sect:rateparams}}, \textbf{\nameref{sect:randomwalks}}, \textbf{\nameref{sect:NER}}, and \textbf{\nameref{AVMVN_sect}}.
The four operators introduced in these sections are summarised in \textbf{Table \ref{table:newOperators}}.
The setting which was considered to be the best in each step was then incorporated into the following step. 
This protocol and its outcomes are summarised in \textbf{Fig \ref{fig:tournament}}.





\begin{figure}[!h]
\includegraphics[width=\textwidth]{Figures/tournament.pdf}
\caption{\textbf{Protocol for optimising clock model methodologies.} Each area (detailed in \textbf{\nameref{sect:models}}) is optimised sequentially, and the best setting from each step is used when optimising the following step.}
\label{fig:tournament}
\end{figure}


Methodologies were assessed according to the following criteria.


\textbf{1. Validation}. This was assessed by measuring the coverage of all estimated parameters in well-calibrated simulation studies. 
These are presented in \textbf{\nameref{sect:WCSS_appendix}} and give confidence operators are implemented correctly. \\


\textbf{2. Mixing of parameters}. Key parameters were evaluated for the number of effective samples generated per hour (ESS/hr).
These key parameters were the likelihood $\mathcal{L}$ and prior $p$ densities,  tree length $l$ (i.e. the sum of all branch lengths), mean branch rate $\bar{r}$, branch rate of all leaf nodes $r$, and relaxed clock standard deviation $\sigma$.
We also included the HKY substitution model term $\kappa$. 
The mixing of $\kappa$ should not be strongly affected by any of the clock model operators, and thus it served as a positive control in each experiment. \\




Methodologies were benchmarked using one simulated and eight empirical datasets.
The latter were compiled \cite{lanfear2019Github} and partitioned \cite{lanfear2016partitionfinder} by Lanfear as ``benchmark alignments'' (\textbf{Table \ref{table:datasets}}).
Each methodology was benchmarked for million-states-per-hour using the Intel Xeon Gold 6138 CPU (2.00 GHz).
These terms were multiplied by the ESS-per-state across 20 replicates on the New Zealand eScience Infrastructure (NeSI) cluster to compute the total ESS/hr of each dataset under each setting.
All methodologies used identical models and operator configurations, except where a difference is specified.


%All 25 datasets presented in \textbf{Table \ref{table:datasets}} are benchmarked together $1 \times$ each at the end of the protocol. Datasets and BEAST 2 .xml templates are provided in the GitHub repository accompanying this article.



% https://github.com/roblanf/BenchmarkAlignments
\begin{table}[h!]
\centering
\begin{tabular}{l l l l l l l} 
  & $N$ & $P$ & $L$ (kb) & $L_\text{unq}$ (kb) & $\hat{\sigma}$ & \textbf{Description} \\
  \hline
 
 
 1  &  38  &  16  &  15.5  &  10.5 &  0.44 &  Seed plants (Ran 2018 \cite{Ran_2018}) \\ 

2  &  44  &  7  &  5.9  &  1.8& 0.30 &  Squirrel Fishes (Dornburg 2012 \cite{Dornburg_2012}) \\ 

3  &  44  &  3  &  1.9  &  0.8& 0.27  &  Bark beetles (Cognato 2001 \cite{Cognato_2001}) \\ 

4  &  51  &  6  &  5.4  &  1.8 & 0.52 &  Southern beeches (Sauquet 2011 \cite{Sauquet_2011}) \\ 

5  &  61  &  8  &  6.9  &  4.3 & 0.53 &  Bony fishes (Broughton 2013 \cite{Broughton_2013}) \\ 

6  &  70  &  3  &  2.2  &  0.9  & 0.19 &  Caterpillars (Kawahara 2013 \cite{Kawahara_2013}) \\ 

7  &  80  &  1 &  10.0  &  0.9 & 0.82 &  Simulated data  \\ 

8  &  94  &  4  &  2.2  &  1 & 0.34 &  Bees (Rightmyer 2013 \cite{Rightmyer_2013}) \\ 

9  &  106  &  1  &  0.8  &  0.5 & 0.37 &  Songbirds (Moyle 2016 \cite{Moyle_2016}) \\ 



\end{tabular}
\caption{Benchmark datasets, sorted in increasing order of taxon count $N$. Number of partitions $P$, total alignment length $L$, and number of unique site patterns $L_\text{unq}$ in the alignment are also specified.
Clock standard deviation estimates $\hat\sigma$ are of moderate magnitude, suggesting that most of these datasets are not clock-like.
}
\label{table:datasets}
\end{table}


\clearpage
\subsection*{Round 1: A simple operator-weight learning algorithm greatly improved performance}

We compared the nocons, cons, and adapt operator configurations (\textbf{Table \ref{table:operatorSchemes}}).
nocons contained all of the standard BEAST 2 operator configurations and weightings for \textit{real}, \textit{cat}, and \textit{quant}.
cons additionally contained (cons)tant distance operators and employed the same operator weighting scheme used previously \cite{zhang2020improving} (\textit{real} and \textit{quant} only). 
Finally, the adapt configuration combined all of the above applicable operators, as well as the simple-but-bold \texttt{SampleFromPrior} operator, and learned the weights of each operator using the  \texttt{AdaptiveOperatorSampler}.



This experiment revealed that nocons usually performed better than cons on smaller datasets (i.e. small $L$) while cons consistently performed better on larger datasets (\textbf{Fig \ref{fig:round1Results}} and \textbf{\nameref{sect:SR_appendix}}). 
This result is unsurprising (\textbf{Fig \ref{fig:landscape}}).
Furthermore, the adapt setup dramatically improved mixing for \textit{real} by finding the right balance between cons and nocons.
This yielded an ESS/hr (averaged across all 9 datasets) 95\% faster than cons and 520\% faster than nocons, with respect to leaf branch rates, and 620\% and 190\% faster for $\sigma$.
Similar results were observed with \textit{quant}.
However, adapt neither helped nor harmed \textit{cat}, suggesting that the default operator weighting scheme was sufficient.


This experiment also revealed that the standard \texttt{Scale} operator was preferred over \texttt{CisScale} for the \textit{real} configuration. Averaged across all datasets, the learned weights behind these two operators were 0.47 and 0.03.
This was due to the computationally demanding nature of \texttt{CisScale} which invokes the i-CDF function.
In contrast, the performance of \texttt{Scale} and \texttt{CisScale} were more similar under the \textit{quant} configuration and were weighted at 0.28 and 0.45.
For both \textit{real} and \textit{quant}, proposals which altered quantiles, while leaving the rates constant (\texttt{Scale} and \texttt{CisScale} respectively), were preferred.


Overall, the \texttt{AdaptiveOperatorSampler} operator was included in all subsequent rounds in the tournament. 



\begin{figure}[!h]
\includegraphics[width=\textwidth]{benchmarking/benchmarkingVM/ESS_round1_real.pdf}
\caption{\textbf{Round 1: benchmarking the \texttt{AdaptiveOperatorSampler} operator.} Top left, top right, bottom left: each plot compares the ESS/hr ($\pm 1$ standard error) across two operator configurations.
Bottom right: the effect of sequence length $L$ on operator weights learned by \texttt{AdaptiveOperatorSampler}. Both sets of observations are fit by logistic regression models.
 The benchmark datasets are displayed in \textbf{Table \ref{table:datasets}}. The \emph{cat} and \emph{quant} settings are evaluated in \textbf{\nameref{sect:SR_appendix}}.  }
\label{fig:round1Results}
\end{figure}








\clearpage
\subsection*{Round 2: The \textit{real} parameterisation yielded the fastest mixing }

We compared the three rate parameterisations described in \textbf{\nameref{sect:rateparams}}. 
adapt (\textit{real}) and adapt (\textit{quant}) both employed constant distance tree operators \cite{zhang2020improving} and both used the \texttt{AdaptiveOperatorSampler} operator to learn clock model operator weights.
Clock model operators weights were also learned in the adapt (\textit{cat}) configuration.




This experiment showed that the \textit{real} parameterisation greatly outperformed \textit{cat} on most datasets and most parameters (\textbf{Fig \ref{fig:round2Results}}).
This disparity was strongest for long alignments.
In the most extreme case, leaf substitution rates $r$ and clock standard deviation $\sigma$ both mixed around $50\times$ faster on the 15.5 kb seed plant dataset (Ran et al. 2018 \cite{Ran_2018}) for \textit{real} than they did for \textit{cat}.
The advantages in using constant distance operators would likely be even stronger for larger $L$.
Furthermore, \textit{real} outperformed \textit{quant} on most datasets, but this was mostly due to the slow computational performance of \textit{quant} compared with real, as opposed to differences in mixing prowess (\textbf{Fig \ref{fig:round2Resultsb}}).
Irrespective of mixing ability, the adapt (\textit{real}) configuration had the best computational performance and generated samples 40\% faster than adapt (\textit{cat}) and 60\% faster than adapt (\textit{quant}).


Overall, we determined that \textit{real}, and its associated operators, made the best parameterisation covered here and it proceeded to the following rounds of benchmarking.



\begin{figure}[!h]
\includegraphics[width=\textwidth]{benchmarking/benchmarkingVM/ESS_round2.pdf}
\caption{\textbf{Round 2: benchmarking substitution rate parameterisations.} Top left, top right, bottom left: the adapt (\textit{real}), adapt (\textit{cat}), and adapt (\textit{quant}) configurations were compared. Bottom right: comparison of the mean tip substitution rate ESS/hr as a function of alignment length $L$. }
\label{fig:round2Results}
\end{figure}




\begin{figure}[!h]
\includegraphics[width=\textwidth]{benchmarking/benchmarkingVM/ESS_timesettings.pdf}
\caption{\textbf{Comparison of runtimes across methodologies.} 
The computational time required for a setting to sample a single state is divided by that of the nocons (\textit{cat}) configuration.
The geometric mean under each configuration, averaged across all 9 datasets, is displayed as a horizontal bar.}
\label{fig:round2Resultsb}
\end{figure}




% 1-3) the \textit{cat}, \textit{real}, and \textit{quant} parameterisations using the operators described by Drummond et al. 2006 \cite{drummond2006relaxed}; 3) the \textit{quant} parameteristion 



\subsection*{Round 3: Bactrian proposal kernels were around 15\% more efficient than uniform kernels}


We benchmarked the adapt (\textit{real}) configuration with a) standard uniform proposal kernels, and b) Bactrian(0.95) kernels \cite{yang2013searching}.
These kernels applied to all clock model operators (\textbf{Table \ref{table:bactriankernels}}).
These results confirmed that the Bactrian kernel yields faster mixing than the standard uniform kernel (\textbf{Fig \ref{fig:round3Results}}).
All relevant continuous parameters considered had an ESS/hr, averaged across the 9 datasets, between 15\% and 20\% faster compared with the standard uniform kernel.
Although the Bactrian proposal made little-to-no difference to the caterpillar dataset (Kawahara et al. 2013 \cite{Kawahara_2013}), every other dataset did in fact benefit.
Bactrian proposal kernels proceeded to round 4 of the relaxed clock model optimisation protocol. 




\begin{figure}[!h]
\includegraphics[width=\textwidth]{benchmarking/benchmarkingVM/ESS_round3.pdf}
\caption{\textbf{Round 3: benchmarking the Bactrian kernel.}
The ESS/hr ($\pm 1$ s.e.) under the Bactrian configuration, divided by that under the uniform kernel, is shown in the y-axis for each dataset and relevant parameter.
Horizontal bars show the geometric mean under each parameter.
 }
\label{fig:round3Results}
\end{figure}




\subsection*{Round 4: NER operators outperformed on larger datasets}


Our initial screening of the \texttt{NarrowExchangeRate} (NER) operators revealed that the $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$ operator outperformed the standard  \texttt{NarrowExchange} / $\text{NER} \{\}$ operator about 25\% of the time on simulated data, however it was also very sensitive to the dataset. 
Therefore we wrapped up the two operators ($\text{NER} \{\}$ and $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$) within an \texttt{AdaptiveOperatorSampler} operator so that the appropriate weights could be learned.
In this round we benchmarked the Bactrian + adapt (\textit{real}) setting with the adaptive $\text{NER}$ operator (\textbf{Table \ref{table:operatorSchemes}}).
The benchmark datasets are fairly non clock-like and therefore could potentially benefit from NER (\textbf{Table \ref{table:datasets}}).   


Our experiments confirmed that $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$ was indeed superior on larger datasets (where $L > 5$kb; \textbf{Fig \ref{fig:round4Results}}).
While there was no significant difference in the ESS/hr of continuous parameters, $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$ did have an acceptance rate 41\% higher than that of the standard \texttt{NarrowExchange} operator in the most extreme case (the bony fish alignment by Broughton et al.\cite{Broughton_2013}).
The moderate variance in branch substitution rates ($\hat{\sigma} = 0.5$), coupled with a long alignment (7kb), and high topological uncertainty (\textbf{Fig \ref{fig:parameterisationResults}}) made this dataset the perfect target. 
Every acceptance of a branch rearrangement proposal yields a new topology and thus facilitates traversal of tree space. 


In contrast, the standard \texttt{NarrowExchange} operator outperformed on smaller datasets.
The new operator was not always helpful and sometimes it even hindered performance.
Use of an adaptive operator (\texttt{AdaptiveOperatorSampler}) removes the burden from the user in making the decision of which operator to use.
The \texttt{AdaptiveOperatorSampler(NER)} operator proceeded into the final round of the tournament.



\begin{figure}[!h]
\includegraphics[width=\textwidth]{benchmarking/benchmarkingVM/ESS_round4.pdf}
\caption{\textbf{Round 4: benchmarking the NER operators.} 
The learned weights (left) behind the two NER operators ($\text{NER} \{\}$ and $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$), and the relative difference between their acceptance rates $\alpha$ (right), are presented as functions of sequence length.
Logistic and logarithmic regression models are shown, respectively. }
\label{fig:round4Results}
\end{figure}




\begin{figure}[!h]
%\includegraphics[width=\textwidth]{Figures/cognato.pdf}
\includegraphics[width=\textwidth]{Figures/broughton.pdf}
\caption{\textbf{Maximum clade credibility tree of bony fishes.} 
Branches are coloured by substitution rate (units: substitutions per site per unit of time) and the y-axis shows time, such that there is on average 1 substitution per unit of time.
Internal nodes are labelled with posterior clade support. 
This alignment (Broughton et al. 2013 \cite{Broughton_2013}) received the strongest boost from the $\text{NER}\{\mathcal{D}_{AE}, \mathcal{D}_{BE}, \mathcal{D}_{CE}\}$ operator, likely due to its high topological uncertainty and branch rate variance.   
Tree generated by UglyTrees \cite{uglytrees}.  }
\label{fig:parameterisationResults}
\end{figure}




\subsection*{Round 5: The AVMVN leaf rate operator was computationally demanding and improved mixing very slightly}


We tested the applicability of the AVMVN kernel to leaf rate proposals.
This operator exploits any correlations which exist between leaf branch substitution rates.
To do this, we wrapped the \texttt{LeafAVMVN} operator within an \texttt{AdaptiveOperatorSampler} (\textbf{Table \ref{table:AVMVNoperators}}).
The two configurations compared here were a) adapt + Bactrian + NER (\textit{real}) and b) AVMVN + NER + Bactrian + adapt (\textit{real})  (\textbf{Table \ref{table:operatorSchemes}}).


These results showed that the AVMVN operator yielded slightly better mixing (around 6\% faster) for the tree likelihood, the tree length, and the mean branch rate (\textbf{Fig \ref{fig:round5Results}}).
However, it also produced slightly slower mixing for $\kappa$, reflecting the high computational costs associated with the \texttt{LeafAVMVN} operator (\textbf{Fig \ref{fig:round2Resultsb}}).
The learned weight of the \texttt{LeafAVMVN} operator was quite small (ranging from 1 to 8\% across all datasets), again reflecting its costly nature, but also reinforcing the value in having an adaptive weight operator which penalises slow operators.
The \texttt{LeafAVMVN} operator provided some, but not much, benefit in its current form. 


\begin{figure}[!h]
\includegraphics[width=\textwidth]{benchmarking/benchmarkingVM/ESS_round5.pdf}
\caption{\textbf{Round 5: benchmarking the \texttt{LeafAVMVN} operator.} See \textbf{Fig \ref{fig:round3Results}} caption for figure notation.  }
\label{fig:round5Results}
\end{figure}



Overall, we determined that the AVMVN operator configuration was the final winner of the tournament, however its performance benefits were minor and therefore the computational complexities introduced by the \texttt{LeafAVMVN} operator may not be worth the trouble.



\subsection*{Tournament conclusion}
 In conjunction with all settings which came before it, the tournament winner  outperformed both the historical \textit{cat} configuration \cite{drummond2006relaxed} as well as the recently developed cons (\textit{real}) scheme \cite{zhang2020improving}.
Averaged across all datasets, this configuration yielded a relaxed clock  mixing rate between 1.2 and 13 times as fast as \textit{cat} and between 1.8 and 7.8 times as fast as cons (\textit{real}), depending on the parameter.
For the largest dataset considered (seed plants by Ran et al. 2018 \cite{Ran_2018}), the new settings were up to 66 and 37 times as fast respectively.
This is likely to be even more extreme for larger alignments.



\section*{Discussion} \label{sect:discussion}






% Points to cover

\subsection*{Modern operator design}


Adaptability and advanced proposal kernels, such as Bactrian kernels, are increasingly prevalent in MCMC operator design \cite{haario2001adaptive,vihola2012robust,benson2018adaptive,davis2020blocking}.
Adaptive operators undergo training to improve their efficiency over time \cite{roberts2007coupling}. 
In previous work, the conditional clade probabilities of neighbouring trees have served as the basis of adaptive tree operators \cite{hohna2012guided,meyer2019adaptive}.
Proposal step sizes can be tuned during MCMC \cite{rosenthal2011optimal}.
The mirror kernel learns a target distribution which acts as a ``mirror image'' of the current point \cite{thawornwattana2018designing}.
The AVMVN operator learns correlations between numerical parameters in order to traverse the joint posterior distribution efficiently \cite{baele2017adaptive}.



Here, we introduced an adaptive operator which learns the weights of other operators, by using a target function that rewards operators which bring about large changes to the state and penalises operators which exhibit poor computational runtime (\textbf{Eq \ref{eqn:adaptiveSampler}}).
We demonstrated how learning the operator weightings, on a dataset-by-dataset basis, can improve mixing by up to an order of magnitude.
We also demonstrated the versatility of this operator by applying it to a variety of settings.
Assigning operator weights is an important task in Bayesian MCMC inference and the use of such an operator can relieve some of the burden from the person making this decision.
However, this operator is no silver bullet and it must be used in a way that maintains ergodicity within the search space \cite{roberts2007coupling}.


We also found that a Bactrian proposal kernel quite reliably increased mixing efficiency by 15--20\% (\textbf{Fig \ref{fig:round3Results}}).
Similar observations were made by Yang et al. \cite{yang2013searching}.
While this may only be a modest improvement, incorporation of the Bactrian kernels into pre-existing operators is a computationally straightforward task and we recommend implementing them in Bayesian MCMC software packages.

	
	
	

\subsection*{Traversing tree space}


In this article we introduced the family of narrow exchange rate operators (\textbf{Fig \ref{fig:narrowexchange}}).
These operators are built on top of the narrow exchange operator and are specifically designed for the relaxed clock model, by accounting for the correlation which exists between branch lengths and branch substitution rates. 
This family consists of 48 variants, each of which conserves a unique subset of genetic distances before and after the proposal. 
While most of these operators turned out to be worse than narrow exchange, a small subset were more efficient, but only on large datasets.  


Lakner et al. 2008 categorised tree operators into two classes.
``Branch-rearrangement'' operators relocate a branch and thus alter tree topology. 
Members of this class include narrow exchange, nearest neighbour interchange, and subtree-prune-and-regraft \cite{semple2003phylogenetics}.
Whereas ``branch-length'' operators propose branch lengths, but can potentially alter the tree topology as a side-effect.
 Such operators include subtree slide \cite{hohna2008clock}, LOCAL \cite{simon1998local}, and continuous change \cite{jow2002bayesian}.
Lakner et al. 2008 observed that topological proposals made by the former class consistently outperformed topological changes invoked by the latter  \cite{lakner2008efficiency}.


%found that tree operators which perturb topology consistently perform better than those which also change branch lengths as well as topology .


We hypothesised that the increased efficiency behind narrow exchange rate operators could facilitate proposing internal node heights in conjunction with branch rearrangements.
This would enable the efficient exploration of both topology and branch length spaces with a single proposal.
Unfortunately, by incorporating a random walk on the height of the node being relocated, the acceptance rate of the operator declined dramatically  (\textbf{Fig \ref{fig:acceptanceRateScreening}}). 
This decline was greater when more genetic distances were conserved.


These findings support Lakner's hypothesis.
The design of operators which are able to efficiently traverse topological and branch length spaces simultaneously remains an open problem.

	
	
\subsection*{Larger datasets require smarter operators}

As signal within the dataset becomes stronger, the posterior distribution becomes increasingly peaked (\textbf{Fig \ref{fig:landscape}}).
This change in the posterior topology necessitates the use of operators which exploit known correlations in the posterior density; operators such as AVMVN \cite{baele2017adaptive}, constant distance \cite{zhang2020improving}, and the narrow exchange rate operators introduced in this article.

We have shown that while the latter two operators are efficient on large alignments, they are also quite frequently outperformed by simple random walk operators on small alignments.
For instance, we found that constant distance operators outperformed standard operator configurations by up to two orders of magnitude on larger datasets but they were up to three times slower on smaller ones (\textbf{Fig \ref{fig:round1Results}}). 
Similarly, our narrow exchange rate operators were up to 40\% more efficient on large datasets but up to 10\% less efficient on smaller ones (\textbf{Fig \ref{fig:round4Results}}).

This emphasises the value in our adaptive operator weighting scheme, which can ensure that operator weights are suitable for the size of the alignment.
Given the overwhelming availability of sequence data, high performance on large datasets is more important than ever.



\section*{Conclusion}

In this article, we delved into the highly correlated structure between substitution rates and divergence times of relaxed clock models, in order to develop MCMC operators which traverse its posterior space efficiently.
We introduced a range of relaxed clock model operators and compared three molecular substitution rate parameterisations.
These methodologies were compared by constructing phylogenetic models from several empirical datasets and comparing their abilities to converge in a tournament-like protocol (\textbf{Fig \ref{fig:tournament}}).
The methods introduced are adaptive, treat each dataset differently, and rarely perform worse than without adaptation. 
This work has produced an operator configuration which is highly effective  on large alignments and can explore relaxed clock model space up to two orders of magnitude more efficiently than previous setups.

%Averaged across all datasets, this configuration yielded a relaxed clock  mixing rate between 1.2 and 13 times as fast as \textit{cat} and between 1.8 and 7.8 times as fast as cons (\textit{real}), depending on the parameter.
%For the largest dataset considered (seed plants by Ran et al. 2018 \cite{Ran_2018}), the new settings were up to 66 and 37 times as fast respectively.
%This is likely to be even more extreme for larger alignments.


\section*{Software availability}

The work presented in this article was implemented as a user friendly open source BEAST 2 package with GUI support via BEAUti making it easy to set up an analysis. Instructions for using and installing this package can be found at \href{https://github.com/jordandouglas/ORC}{https://github.com/jordandouglas/ORC}.



\section*{Acknowledgements}

We wish to thank Alexei J. Drummond for his help during conception of the narrow exchange rate operators.



\section*{Supporting information}


\paragraph*{S1 Appendix.}
\label{sect:S1Appendix}
{\textbf{Rate quantiles.} The linear piecewise approximation used in the \textit{quant} parameterisation is described. 
Constant distance tree operators, \texttt{CisScale}, and  \texttt{NarrowExchangeRate} are extended to the \textit{quant} parameterisation. 



\paragraph*{S2 Appendix.}
\label{sect:WCSS_appendix}
{\textbf{Well-calibrated simulation studies.} Methodologies are validated using well-calibrated simulation studies.



\paragraph*{S1 Fig.}
\label{sect:SR_appendix}
{\textbf{Round 1: benchmarking adapt under the \textit{quant} and \textit{cat} parameterisations.} These results show that \textit{cat} does not benefit
from adaptive weight sampling. Whereas, adapt and cons both greatly improve the \textit{quant}
parameterisation for most datasets, as expected.




\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for
% step-by-step instructions.
%

\newpage
\bibliography{references}



\end{document}















